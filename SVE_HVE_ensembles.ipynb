{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load prediction from fine-tuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first load the text and gold labels\n",
    "input = pd.read_csv('finetuned-models-output/fBERT_in_output_predictions.csv', delimiter=';')\n",
    "full_results = input[['text', 'gold']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the probabilities were stored as strings, revert them to lists of floats\n",
    "def fix_prob(val):\n",
    "    \"\"\"Probability is seen as a full string, extract actual floats from string\"\"\"\n",
    "    x = val.replace('[',\"\").replace(']',\"\").split()\n",
    "    new_prob = [float(x[0]), float(x[1])]\n",
    "    return new_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then add the predicitons as new columns\n",
    "for model in ['bert-base-uncased', 'fBERT', 'hateBERT']:\n",
    "    for domain in ['in','cross']:\n",
    "        data = pd.read_csv(f'fine-tune results/{model}_{domain}_output_predictions.csv', delimiter=';')\n",
    "        full_results[f'prediction_{model}_{domain}'] = data['prediction']\n",
    "        full_results[f'probability_{model}_{domain}'] = data['probability'].transform(fix_prob) # apply float fix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start ensemble calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the functions for calculating the voting\n",
    "\n",
    "def soft_major_in(row):\n",
    "    # sum all probabilities\n",
    "    summed = np.sum([\n",
    "                    row['probability_bert-base-uncased_in'],\n",
    "                    row['probability_fBERT_in'],\n",
    "                    row['probability_hateBERT_in']\n",
    "                    ],\n",
    "                    axis=0)\n",
    "    # return the max score as final prediction\n",
    "    final_soft_pred = 0 if summed[0] > summed[1] else 1\n",
    "    return final_soft_pred\n",
    "\n",
    "def hard_major_in(row):\n",
    "    # sum all predictions\n",
    "    summed = np.sum([\n",
    "                row['prediction_bert-base-uncased_in'],\n",
    "                row['prediction_fBERT_in'],\n",
    "                row['prediction_hateBERT_in']\n",
    "                ], axis=0)\n",
    "    # take average and return final prediction\n",
    "    if summed/3 < 0.5:\n",
    "        return 0\n",
    "    else: return 1\n",
    "\n",
    "\n",
    "def soft_major_cross(row):\n",
    "    summed = np.sum([\n",
    "                    row['probability_bert-base-uncased_cross'],\n",
    "                    row['probability_fBERT_cross'],\n",
    "                    row['probability_hateBERT_cross']\n",
    "                    ],\n",
    "                    axis=0)\n",
    "    final_soft_pred = 0 if summed[0] > summed[1] else 1\n",
    "    return final_soft_pred\n",
    "\n",
    "def hard_major_cross(row):\n",
    "    summed = np.sum([\n",
    "                row['prediction_bert-base-uncased_cross'],\n",
    "                row['prediction_fBERT_cross'],\n",
    "                row['prediction_hateBERT_cross']\n",
    "                ], axis=0)\n",
    "    if summed/3 < 0.5:\n",
    "        return 0\n",
    "    else: return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply calculations and store in new column\n",
    "full_results['soft_major_in'] = full_results.apply(soft_major_in, axis=1)\n",
    "full_results['hard_major_in'] = full_results.apply(hard_major_in, axis=1)\n",
    "full_results['soft_major_cross'] = full_results.apply(soft_major_cross, axis=1)\n",
    "full_results['hard_major_cross'] = full_results.apply(hard_major_cross, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get lists of final predictions to get easy acces to performance metrics\n",
    "gold = full_results.gold\n",
    "target_names = ['NOT OFF', 'OFF']\n",
    "\n",
    "soft_pred_in = full_results.soft_major_in\n",
    "hard_pred_in = full_results.hard_major_in\n",
    "\n",
    "soft_pred_cross = full_results.soft_major_cross\n",
    "hard_pred_cross = full_results.hard_major_cross\n",
    "\n",
    "bert_in = full_results['prediction_bert-base-uncased_in']\n",
    "bert_cross = full_results['prediction_bert-base-uncased_cross']\n",
    "\n",
    "fbert_in = full_results.prediction_fBERT_in\n",
    "fbert_cross = full_results.prediction_fBERT_cross\n",
    "\n",
    "hatebert_in = full_results.prediction_hateBERT_in\n",
    "hatebert_cross = full_results.prediction_hateBERT_cross"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the final results (all fine-tuned models and soft- hard voting ensembles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_results_out = pd.DataFrame(zip(full_results.text, gold, bert_in, bert_cross,\n",
    "                                    fbert_in, fbert_cross, hatebert_in, hatebert_cross,\n",
    "                                    soft_pred_in, soft_pred_cross, hard_pred_in, hard_pred_cross),\n",
    "                                    columns=[\n",
    "                                    'text', 'gold', 'bert_in', 'bert_cross',\n",
    "                                    'fbert_in', 'fbert_cross', 'hatebert_in', 'hatebert_cross',\n",
    "                                    'soft_pred_in', 'soft_pred_cross', 'hard_pred_in', 'hard_pred_cross'])\n",
    "full_results_out.head()\n",
    "full_results_out.to_csv('ensemble_output_all_models.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print all classification reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(gold, bert_in, target_names=target_names))\n",
    "print(classification_report(gold, bert_cross, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(gold, fbert_in, target_names=target_names))\n",
    "print(classification_report(gold, fbert_cross, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(gold, hatebert_in, target_names=target_names))\n",
    "print(classification_report(gold, hatebert_cross, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(gold, hatebert_in))\n",
    "print(confusion_matrix(gold, hatebert_cross))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(gold, soft_pred_in, target_names=target_names))\n",
    "print(classification_report(gold, soft_pred_cross, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(gold, soft_pred_in))\n",
    "print(confusion_matrix(gold, soft_pred_cross))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(gold, hard_pred_in, target_names=target_names))\n",
    "\n",
    "print(classification_report(gold, hard_pred_cross, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(gold, hard_pred_in))\n",
    "print(confusion_matrix(gold, hard_pred_cross))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e6f88519aa74d89c62cdcc59414067e256965681425d9b4dee1ef762f1172e6f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
